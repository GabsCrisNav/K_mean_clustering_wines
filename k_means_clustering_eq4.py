# -*- coding: utf-8 -*-
"""K_means_clustering_EQ4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13d0hYjpYFVdYVRSLgk7vzWRUGqZ5X8Zs

# K-Means Clustering con 2 Variables

## Importo mis librerias
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
# %matplotlib inline

#from google.colab import drive
#drive.mount('/content/drive')

"""## Importo mi dataset"""

dataset = pd.read_csv('wine-clustering.csv')

df = dataset.iloc[:, [6, 11]].values
dataset.head()

df.shape

df

"""## Uso el método del codo para saber mi número adecuado de clusters 'k'"""

from sklearn.cluster import KMeans

wcss = [] #within cluster sum of squares

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(df)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

"""## Realizo mi agrupación con base en el elbow plot"""

kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(df)
print(kmeans.labels_)

"""## Visualización de los clusters"""

plt.scatter(df[y_kmeans == 0, 0], df[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Cluster 1')
plt.scatter(df[y_kmeans == 1, 0], df[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Cluster 2')
plt.scatter(df[y_kmeans == 2, 0], df[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Cluster 3')
#plt.scatter(df[y_kmeans == 3, 0], df[y_kmeans == 3, 1], s = 50, c = 'cyan', label = 'Cluster 4')
#plt.scatter(df[y_kmeans == 4, 0], df[y_kmeans == 4, 1], s = 50, c = 'magenta', label = 'Cluster 5')
#plt.scatter(df[y_kmeans == 5, 0], df[y_kmeans == 5, 1], s = 80, c = 'black', label = 'Cluster 6')
#plt.scatter(df[y_kmeans == 6, 0], df[y_kmeans == 6, 1], s = 80, c = 'yellow', label = 'Cluster 7')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 200, c = 'yellow', label = 'Centroids')
plt.title('Clusters of customers')
plt.xlabel('Flavanoids')
plt.ylabel('OD280')
plt.legend()
plt.show()

"""# K-Means clustering con más de dos variables

## Importando las librerías
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importamos el dataset"""

dataset = pd.read_csv('wine-clustering.csv')

columns = ['Flavanoids','OD280','Total_Phenols']
df3 = dataset[columns]
df3

"""## Uso el método del codo para saber mi número adecuado de clusters 'k'"""

from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(df3)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

"""## Entrenando un modelo de k-means para los datos"""

kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(df3)
print(kmeans.labels_)
#labels = kmeans.labels_
#labels
#df['clusters'] = labels
#clmns.extend(['clusters'])

fig = plt.figure(figsize = (15,15))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(df3.iloc[y_kmeans == 0,0],df3.iloc[y_kmeans == 0,1],df3.iloc[y_kmeans == 0,2], s = 40 , color = 'blue', label = "cluster 0")
ax.scatter(df3.iloc[y_kmeans == 1,0],df3.iloc[y_kmeans == 1,1],df3.iloc[y_kmeans == 1,2], s = 40 , color = 'orange', label = "cluster 1")
ax.scatter(df3.iloc[y_kmeans == 2,0],df3.iloc[y_kmeans == 2,1],df3.iloc[y_kmeans == 2,2], s = 40 , color = 'green', label = "cluster 2")
#ax.scatter(df3.iloc[y_kmeans == 3,0],df3.iloc[y_kmeans == 3,1],df3.iloc[y_kmeans == 3,2], s = 40 , color = '#D12B60', label = "cluster 3")
#ax.scatter(df3.iloc[y_kmeans == 4,0],df3.iloc[y_kmeans == 4,1],df3.iloc[y_kmeans == 4,2], s = 40 , color = 'purple', label = "cluster 4")
ax.set_xlabel('Flavanoids-->')
ax.set_ylabel('OD280-->')
ax.set_zlabel('Total Phenols-->')
ax.legend()
plt.show()

"""#Preguntas

1. Si utilizaras más de dos variables, ¿Cuáles seleccionarías? Justifica tu respuesta.

Los dos que decidimos utilizar fueron las variables de 'Flavanoids' y 'OD280'. Como se puede ver en la gráfica de 3D, la variable adicional que elegimos es el de 'Total Phenols' ya que este, como las otras dos, tiene de las correlaciones más altas con alguna de las demás variables. Esto apreciar en mapa de calor de correlaciones.
"""

correlations=dataset.corr().abs()
sns.heatmap(correlations)

"""2. ¿Crees que estos centros puedan ser representativos de los datos? ¿Por qué?

Los centros son una generalización de los datos, pero no representan los datos completos, ya estos estan dispersos apesar de tener una buena correlación.

3. ¿Cómo obtuviste el valor de k a usar?

El valor de 'k' en cada gráfica se calculó utilizando el método del codo.

4. ¿Tu valor de k sería mejor si utilizaras un valor más alto o más bajo?

Consideramos que el valor de 'k' que utilizamos en las gráficas es suficientemente adecuada para la representación de los diferentes clusters de datos.

5. ¿Hay centros que estén más cerca de otros?

Sí, el centro del segundo cluster está más cercano del centro del tercer cluster que del primero.

6. ¿Qué pasaría con los clusters si tuviéramos muchos outliers en el análisis de cajas y bigotes?

Los centros cambiarían de posición, por lo que las formas y los tamaños de los clusters también cambiarían.

7. ¿Qué puedes decir de los datos basándote en los clusters? ¿Qué conclusión puedes tener?

Los tres clusters encontrados podemos darles los lables de vino de mesa, vino de crianza y vino de reserva.

El vino de mesa, o vino jóven, es el menos añejado. El vino de crianza es el siguiente, siendo añejado por un mínimo de 24 meses. El último, el vino de reserva, debe tener un añejado mínimo de 3 años.

Los vinos más jóvenes por lo general tienen sabores más agrios y más astringencia. Se sabe que el flavonoide afecta la astringencia del vino. Mientras más flavonoides, más astringente.

Tomando en cuenta todo lo anterior, se puede concluir que el primer cluster, con menor cantidad de las tres variables, es el vino de reserva. El cluster de en medio es el vino de crianza. Finalmente, el vino de mesa es el que tiene más cantidad de las tres variables.
"""